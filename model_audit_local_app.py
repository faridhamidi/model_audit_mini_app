#!/usr/bin/env python3
"""Standalone local mini app for Codex model routing and usage audit."""

from __future__ import annotations

import argparse
import csv
import gzip
import json
import re
import sys
import tempfile
from dataclasses import dataclass
from datetime import datetime, timedelta, timezone
from http import HTTPStatus
from http.server import BaseHTTPRequestHandler, ThreadingHTTPServer
from pathlib import Path
from typing import Any
from urllib.parse import parse_qs, urlparse

try:
    from .metrics_audit import build_usage_audit_report
except ImportError:  # pragma: no cover - support direct script execution
    from metrics_audit import build_usage_audit_report


CSV_HEADERS = [
    "timestamp_utc",
    "timestamp_epoch_ms",
    "event_type",
    "thread_id",
    "thread_label",
    "conversation_id",
    "query_id",
    "query_index",
    "model",
    "cwd",
    "sandbox_mode",
    "approval_policy",
    "session_file",
    "message_role",
    "message_text",
    "query_text",
    "tool_name",
    "tool_call_id",
    "tool_start_epoch_ms",
    "tool_end_epoch_ms",
    "tool_duration_ms",
    "input_tokens",
    "output_tokens",
    "cached_input_tokens",
    "reasoning_output_tokens",
    "total_tokens_raw",
    "total_tokens_recomputed",
    "total_tokens_delta",
    "reconciliation_status",
    "total_tokens",
]

HTML_NAME = "model_audit_dashboard.html"
AUDIT_JSON_NAME = "model_audit_audit.json"
SUMMARY_JSON_NAME = "model_audit_summary.json"
OPPORTUNITIES_JSON_NAME = "model_audit_opportunities.json"
DATA_DIR_NAME = "data"
DATA_CATALOG_NAME = "catalog.json"
CODEX_GLOBAL_STATE_PATH = Path.home() / ".codex" / ".codex-global-state.json"
DEFAULT_MAX_CSV_ROWS = 0
DEFAULT_MAX_RETENTION_DAYS = 31
MAX_QUERY_TEXT_CHARS = 600
MAX_MESSAGE_TEXT_CHARS = 240

ROUTING_CSV_HEADERS = [
    "timestamp_utc",
    "timestamp_epoch_ms",
    "event_type",
    "thread_id",
    "thread_label",
    "conversation_id",
    "query_id",
    "query_index",
    "model",
    "cwd",
    "sandbox_mode",
    "approval_policy",
    "session_file",
]

QUERY_CSV_HEADERS = [
    "timestamp_utc",
    "timestamp_epoch_ms",
    "event_type",
    "thread_id",
    "thread_label",
    "conversation_id",
    "query_id",
    "query_index",
    "model",
    "cwd",
    "sandbox_mode",
    "approval_policy",
    "session_file",
    "message_role",
    "message_text",
    "query_text",
]

TOKEN_CSV_HEADERS = [
    "timestamp_utc",
    "timestamp_epoch_ms",
    "event_type",
    "thread_id",
    "thread_label",
    "conversation_id",
    "query_id",
    "query_index",
    "model",
    "cwd",
    "sandbox_mode",
    "approval_policy",
    "session_file",
    "input_tokens",
    "output_tokens",
    "cached_input_tokens",
    "reasoning_output_tokens",
    "total_tokens_raw",
    "total_tokens_recomputed",
    "total_tokens_delta",
    "reconciliation_status",
    "total_tokens",
]

TOOL_CSV_HEADERS = [
    "timestamp_utc",
    "timestamp_epoch_ms",
    "event_type",
    "thread_id",
    "thread_label",
    "conversation_id",
    "query_id",
    "query_index",
    "model",
    "cwd",
    "sandbox_mode",
    "approval_policy",
    "session_file",
    "tool_name",
    "tool_call_id",
    "tool_start_epoch_ms",
    "tool_end_epoch_ms",
    "tool_duration_ms",
]

DATASET_SPECS: dict[str, dict[str, Any]] = {
    "routing_context": {
        "event_type": "turn_context",
        "file_name": "routing_context.csv",
        "description": "Routing policy snapshots (model/sandbox/approval per turn context).",
        "headers": ROUTING_CSV_HEADERS,
    },
    "query_messages": {
        "event_type": "user_message",
        "file_name": "query_messages.csv",
        "description": "User query messages and normalized query text.",
        "headers": QUERY_CSV_HEADERS,
    },
    "usage_tokens": {
        "event_type": "token_count",
        "file_name": "usage_tokens.csv",
        "description": "Token telemetry and reconciliation metrics per query.",
        "headers": TOKEN_CSV_HEADERS,
    },
    "tool_calls": {
        "event_type": "tool_call",
        "file_name": "tool_calls.csv",
        "description": "Tool invocations and duration timing.",
        "headers": TOOL_CSV_HEADERS,
    },
}

UUID_PATTERN = re.compile(r"[0-9a-f]{8}-[0-9a-f]{4}-[1-5][0-9a-f]{3}-[89ab][0-9a-f]{3}-[0-9a-f]{12}", re.IGNORECASE)
NUMERIC_PATTERN = re.compile(r"\b\d+(?:\.\d+)?\b")
SPACE_PATTERN = re.compile(r"\s+")
QUERY_INDEX_PATTERN = re.compile(r":q(\d+)$")


@dataclass
class AuditEventRow:
    timestamp: datetime
    event_type: str
    thread_id: str
    thread_label: str | None = None
    conversation_id: str | None = None
    query_id: str | None = None
    query_index: int | None = None
    model: str = "unknown"
    cwd: str = "unknown"
    sandbox_mode: str = "unknown"
    approval_policy: str = "unknown"
    session_file: str = ""
    message_role: str | None = None
    message_text: str | None = None
    query_text: str | None = None
    tool_name: str | None = None
    tool_call_id: str | None = None
    tool_start_epoch_ms: int | None = None
    tool_end_epoch_ms: int | None = None
    tool_duration_ms: int | None = None
    input_tokens: int | None = None
    output_tokens: int | None = None
    cached_input_tokens: int | None = None
    reasoning_output_tokens: int | None = None
    total_tokens_raw: int | None = None
    total_tokens_recomputed: int | None = None
    total_tokens_delta: int | None = None
    reconciliation_status: str | None = None
    total_tokens: int | None = None


def parse_timestamp(value: str | None) -> datetime | None:
    if not value:
        return None
    raw = value.replace("Z", "+00:00")
    try:
        dt = datetime.fromisoformat(raw)
    except ValueError:
        return None
    if dt.tzinfo is None:
        dt = dt.replace(tzinfo=timezone.utc)
    return dt.astimezone(timezone.utc)


def as_utc_iso(dt: datetime) -> str:
    return dt.astimezone(timezone.utc).isoformat().replace("+00:00", "Z")


def normalize_sandbox_mode(policy: Any) -> str:
    if isinstance(policy, dict):
        return str(policy.get("type") or policy.get("mode") or "unknown")
    return "unknown"


def coerce_int(value: Any) -> int | None:
    if value is None:
        return None
    if isinstance(value, bool):
        return None
    if isinstance(value, int):
        return value
    if isinstance(value, float):
        return int(value)
    if isinstance(value, str):
        text = value.strip()
        if not text:
            return None
        try:
            if "." in text:
                return int(float(text))
            return int(text)
        except ValueError:
            return None
    return None


def compact_text(value: str | None, max_chars: int) -> str:
    if not value:
        return ""
    normalized = " ".join(value.split())
    if max_chars <= 0:
        return ""
    if len(normalized) <= max_chars:
        return normalized
    if max_chars <= 3:
        return normalized[:max_chars]
    return normalized[: max_chars - 3] + "..."


def extract_response_message_text(payload: dict[str, Any]) -> str:
    content = payload.get("content")
    if isinstance(content, list):
        parts: list[str] = []
        for item in content:
            if not isinstance(item, dict):
                continue
            text = item.get("text")
            if isinstance(text, str) and text.strip():
                parts.append(text)
        return "\n".join(parts).strip()
    text = payload.get("text")
    if isinstance(text, str):
        return text.strip()
    return ""


def parse_tool_duration_ms(output_payload: Any) -> int | None:
    parsed: Any = output_payload
    if isinstance(output_payload, str):
        text = output_payload.strip()
        if text:
            try:
                parsed = json.loads(text)
            except json.JSONDecodeError:
                parsed = None

    if not isinstance(parsed, dict):
        return None

    metadata = parsed.get("metadata")
    if isinstance(metadata, dict):
        dur_ms = coerce_int(metadata.get("duration_ms"))
        if dur_ms is not None:
            return max(0, dur_ms)
        dur_seconds = metadata.get("duration_seconds")
        if isinstance(dur_seconds, (int, float)):
            return max(0, int(dur_seconds * 1000))

    dur_ms = coerce_int(parsed.get("duration_ms"))
    if dur_ms is not None:
        return max(0, dur_ms)
    dur_seconds = parsed.get("duration_seconds")
    if isinstance(dur_seconds, (int, float)):
        return max(0, int(dur_seconds * 1000))
    return None


def load_thread_title_map(global_state_path: Path = CODEX_GLOBAL_STATE_PATH) -> dict[str, str]:
    try:
        raw = global_state_path.read_text(encoding="utf-8")
    except OSError:
        return {}
    try:
        payload = json.loads(raw)
    except json.JSONDecodeError:
        return {}
    if not isinstance(payload, dict):
        return {}
    section = payload.get("thread-titles")
    if not isinstance(section, dict):
        return {}
    titles = section.get("titles")
    if not isinstance(titles, dict):
        return {}
    mapped: dict[str, str] = {}
    for key, value in titles.items():
        if not isinstance(key, str):
            continue
        if not isinstance(value, str):
            continue
        title = value.strip()
        if title:
            mapped[key.strip()] = title
    return mapped


def make_thread_label(first_query_text: str, thread_id: str, cwd: str) -> str:
    lines = [line.strip() for line in first_query_text.splitlines() if line.strip()]
    for line in lines:
        low = line.lower()
        if low.startswith("<environment_context>"):
            continue
        if low.startswith("# context from my ide setup"):
            continue
        if low.startswith("## my request for codex"):
            continue
        cleaned = line.lstrip("-*# ").strip()
        if cleaned:
            return cleaned[:72]

    base = Path(cwd).name if cwd and cwd != "unknown" else "thread"
    suffix = thread_id.split("-")[0][:8] if thread_id else "unknown"
    return f"{base}-{suffix}"[:72]


def list_session_log_files(sessions_dir: Path) -> list[Path]:
    files: set[Path] = set()
    files.update(sessions_dir.rglob("*.jsonl"))
    files.update(sessions_dir.rglob("*.jsonl.gz"))
    return sorted(files)


def derive_thread_id_from_log_file(file_path: Path) -> str:
    file_name = file_path.name
    if file_name.endswith(".jsonl.gz"):
        return file_name[: -len(".jsonl.gz")]
    if file_name.endswith(".jsonl"):
        return file_name[: -len(".jsonl")]
    return file_path.stem


def open_session_log_text(file_path: Path):
    if file_path.name.endswith(".gz"):
        return gzip.open(file_path, mode="rt", encoding="utf-8", errors="replace")
    return file_path.open("r", encoding="utf-8", errors="replace")


def extract_event_rows(sessions_dir: Path) -> list[AuditEventRow]:
    rows: list[AuditEventRow] = []
    if not sessions_dir.exists():
        return rows

    thread_title_map = load_thread_title_map()

    for file_path in list_session_log_files(sessions_dir):
        file_rows: list[AuditEventRow] = []
        session_file_name = file_path.name
        thread_id = derive_thread_id_from_log_file(file_path)
        context_model = "unknown"
        context_cwd = "unknown"
        context_sandbox_mode = "unknown"
        context_approval_policy = "unknown"
        first_query_text = ""
        first_known_cwd = "unknown"

        query_index = 0
        current_query_id = "unknown"
        current_conversation_id = "unknown"

        open_tools: dict[str, dict[str, Any]] = {}

        try:
            with open_session_log_text(file_path) as handle:
                for line in handle:
                    line = line.strip()
                    if not line:
                        continue
                    try:
                        entry = json.loads(line)
                    except json.JSONDecodeError:
                        continue

                    ts = parse_timestamp(entry.get("timestamp"))
                    if ts is None:
                        continue

                    entry_type = entry.get("type")
                    payload = entry.get("payload")
                    if not isinstance(payload, dict):
                        payload = {}

                    if entry_type == "session_meta":
                        session_id = payload.get("id")
                        if session_id:
                            thread_id = str(session_id)
                        continue

                    if entry_type == "turn_context":
                        model = payload.get("model")
                        if model:
                            context_model = str(model)

                        cwd = payload.get("cwd")
                        if cwd:
                            context_cwd = str(cwd)
                            if first_known_cwd == "unknown":
                                first_known_cwd = context_cwd

                        if payload.get("sandbox_policy") is not None:
                            context_sandbox_mode = normalize_sandbox_mode(payload.get("sandbox_policy"))

                        approval = payload.get("approval_policy")
                        if approval:
                            context_approval_policy = str(approval)

                        file_rows.append(
                            AuditEventRow(
                                timestamp=ts,
                                event_type="turn_context",
                                thread_id=thread_id,
                                conversation_id=current_conversation_id,
                                query_id=current_query_id,
                                query_index=query_index if query_index > 0 else None,
                                model=context_model,
                                cwd=context_cwd,
                                sandbox_mode=context_sandbox_mode,
                                approval_policy=context_approval_policy,
                                session_file=session_file_name,
                            )
                        )
                        continue

                    if entry_type == "event_msg":
                        payload_type = payload.get("type")
                        if payload_type == "user_message":
                            query_index += 1
                            current_query_id = f"{thread_id}:q{query_index}"
                            current_conversation_id = current_query_id

                            message_text = str(payload.get("message") or "").strip()
                            message_text = compact_text(message_text, MAX_QUERY_TEXT_CHARS)
                            if not first_query_text and message_text:
                                first_query_text = message_text

                            file_rows.append(
                                AuditEventRow(
                                    timestamp=ts,
                                    event_type="user_message",
                                    thread_id=thread_id,
                                    conversation_id=current_conversation_id,
                                    query_id=current_query_id,
                                    query_index=query_index,
                                    model=context_model,
                                    cwd=context_cwd,
                                    sandbox_mode=context_sandbox_mode,
                                    approval_policy=context_approval_policy,
                                    session_file=session_file_name,
                                    message_role="user",
                                    message_text=message_text,
                                    query_text=message_text,
                                )
                            )
                            continue

                        if payload_type == "token_count":
                            info = payload.get("info")
                            if not isinstance(info, dict):
                                continue
                            last_usage = info.get("last_token_usage")
                            if not isinstance(last_usage, dict):
                                continue

                            input_tokens = coerce_int(last_usage.get("input_tokens"))
                            output_tokens = coerce_int(last_usage.get("output_tokens"))
                            cached_input_tokens = coerce_int(last_usage.get("cached_input_tokens"))
                            reasoning_output_tokens = coerce_int(last_usage.get("reasoning_output_tokens"))
                            total_tokens_raw = coerce_int(last_usage.get("total_tokens"))

                            # Some compacted/session-summary frames emit a non-zero raw total while all
                            # last-usage components are zero. Those rows are not comparable for raw-vs-recomputed
                            # reconciliation and should not fail A1.
                            is_non_comparable_snapshot = (
                                isinstance(total_tokens_raw, int)
                                and total_tokens_raw > 0
                                and (input_tokens or 0) == 0
                                and (output_tokens or 0) == 0
                                and (cached_input_tokens or 0) == 0
                                and (reasoning_output_tokens or 0) == 0
                            )

                            total_tokens_recomputed: int | None = None
                            if not is_non_comparable_snapshot and (input_tokens is not None or output_tokens is not None):
                                total_tokens_recomputed = (input_tokens or 0) + (output_tokens or 0)

                            total_tokens_delta: int | None = None
                            if total_tokens_raw is not None and total_tokens_recomputed is not None:
                                total_tokens_delta = total_tokens_raw - total_tokens_recomputed

                            if is_non_comparable_snapshot:
                                total_tokens = 0
                            elif total_tokens_recomputed is not None:
                                total_tokens = total_tokens_recomputed
                            else:
                                total_tokens = total_tokens_raw

                            if is_non_comparable_snapshot:
                                reconciliation_status = "non_comparable_snapshot"
                            elif total_tokens_delta is None:
                                reconciliation_status = "insufficient_data"
                            elif total_tokens_delta == 0:
                                reconciliation_status = "match"
                            else:
                                reconciliation_status = "mismatch"

                            file_rows.append(
                                AuditEventRow(
                                    timestamp=ts,
                                    event_type="token_count",
                                    thread_id=thread_id,
                                    conversation_id=current_conversation_id,
                                    query_id=current_query_id,
                                    query_index=query_index if query_index > 0 else None,
                                    model=context_model,
                                    cwd=context_cwd,
                                    sandbox_mode=context_sandbox_mode,
                                    approval_policy=context_approval_policy,
                                    session_file=session_file_name,
                                    input_tokens=input_tokens,
                                    output_tokens=output_tokens,
                                    cached_input_tokens=cached_input_tokens,
                                    reasoning_output_tokens=reasoning_output_tokens,
                                    total_tokens_raw=total_tokens_raw,
                                    total_tokens_recomputed=total_tokens_recomputed,
                                    total_tokens_delta=total_tokens_delta,
                                    reconciliation_status=reconciliation_status,
                                    total_tokens=total_tokens,
                                )
                            )
                            continue

                    if entry_type == "response_item":
                        payload_type = payload.get("type")

                        if payload_type == "message":
                            role = str(payload.get("role") or "assistant")
                            if role != "user":
                                continue

                            message_text = extract_response_message_text(payload)
                            if not message_text:
                                continue

                            query_index += 1
                            current_query_id = f"{thread_id}:q{query_index}"
                            current_conversation_id = current_query_id
                            message_text = compact_text(message_text, MAX_QUERY_TEXT_CHARS)
                            if not first_query_text and message_text:
                                first_query_text = message_text

                            file_rows.append(
                                AuditEventRow(
                                    timestamp=ts,
                                    event_type="user_message",
                                    thread_id=thread_id,
                                    conversation_id=current_conversation_id,
                                    query_id=current_query_id,
                                    query_index=query_index if query_index > 0 else None,
                                    model=context_model,
                                    cwd=context_cwd,
                                    sandbox_mode=context_sandbox_mode,
                                    approval_policy=context_approval_policy,
                                    session_file=session_file_name,
                                    message_role=role,
                                    message_text=message_text,
                                    query_text=message_text,
                                )
                            )
                            continue

                        if payload_type == "function_call":
                            call_id = str(payload.get("call_id") or "").strip()
                            if not call_id:
                                continue

                            open_tools[call_id] = {
                                "tool_name": str(payload.get("name") or "unknown_tool"),
                                "start_ts": ts,
                                "thread_id": thread_id,
                                "conversation_id": current_conversation_id,
                                "query_id": current_query_id,
                                "query_index": query_index if query_index > 0 else None,
                                "model": context_model,
                                "cwd": context_cwd,
                                "sandbox_mode": context_sandbox_mode,
                                "approval_policy": context_approval_policy,
                                "session_file": session_file_name,
                            }
                            continue

                        if payload_type == "function_call_output":
                            call_id = str(payload.get("call_id") or "").strip()
                            if not call_id:
                                continue

                            start_info = open_tools.pop(call_id, None)
                            if not start_info:
                                continue

                            start_ts = start_info["start_ts"]
                            start_ms = int(start_ts.timestamp() * 1000)
                            end_ms = int(ts.timestamp() * 1000)
                            duration_ms = parse_tool_duration_ms(payload.get("output"))
                            if duration_ms is None:
                                duration_ms = max(0, end_ms - start_ms)

                            file_rows.append(
                                AuditEventRow(
                                    timestamp=ts,
                                    event_type="tool_call",
                                    thread_id=str(start_info["thread_id"]),
                                    conversation_id=str(start_info["conversation_id"]),
                                    query_id=str(start_info["query_id"]),
                                    query_index=start_info.get("query_index"),
                                    model=str(start_info["model"]),
                                    cwd=str(start_info["cwd"]),
                                    sandbox_mode=str(start_info["sandbox_mode"]),
                                    approval_policy=str(start_info["approval_policy"]),
                                    session_file=str(start_info["session_file"]),
                                    tool_name=str(start_info["tool_name"]),
                                    tool_call_id=call_id,
                                    tool_start_epoch_ms=start_ms,
                                    tool_end_epoch_ms=end_ms,
                                    tool_duration_ms=duration_ms,
                                )
                            )
                            continue

        except OSError:
            continue

        for call_id, start_info in open_tools.items():
            start_ts = start_info["start_ts"]
            start_ms = int(start_ts.timestamp() * 1000)
            file_rows.append(
                AuditEventRow(
                    timestamp=start_ts,
                    event_type="tool_call",
                    thread_id=str(start_info["thread_id"]),
                    conversation_id=str(start_info["conversation_id"]),
                    query_id=str(start_info["query_id"]),
                    query_index=start_info.get("query_index"),
                    model=str(start_info["model"]),
                    cwd=str(start_info["cwd"]),
                    sandbox_mode=str(start_info["sandbox_mode"]),
                    approval_policy=str(start_info["approval_policy"]),
                    session_file=str(start_info["session_file"]),
                    tool_name=str(start_info["tool_name"]),
                    tool_call_id=call_id,
                    tool_start_epoch_ms=start_ms,
                )
            )

        thread_label = thread_title_map.get(thread_id, "").strip()
        if not thread_label:
            thread_label = make_thread_label(
                first_query_text=first_query_text,
                thread_id=thread_id,
                cwd=first_known_cwd,
            )
        for row in file_rows:
            row.thread_label = thread_label
        rows.extend(file_rows)

    rows.sort(key=lambda item: item.timestamp)
    return rows


def _csv_value(value: Any) -> str | int:
    if value is None:
        return ""
    if isinstance(value, bool):
        return "1" if value else "0"
    if isinstance(value, int):
        return value
    return str(value)


def _row_to_csv_dict(row: AuditEventRow) -> dict[str, str | int]:
    return {
        "timestamp_utc": as_utc_iso(row.timestamp),
        "timestamp_epoch_ms": int(row.timestamp.timestamp() * 1000),
        "event_type": row.event_type,
        "thread_id": _csv_value(row.thread_id),
        "thread_label": _csv_value(row.thread_label),
        "conversation_id": _csv_value(row.conversation_id),
        "query_id": _csv_value(row.query_id),
        "query_index": _csv_value(row.query_index),
        "model": _csv_value(row.model),
        "cwd": _csv_value(row.cwd),
        "sandbox_mode": _csv_value(row.sandbox_mode),
        "approval_policy": _csv_value(row.approval_policy),
        "session_file": _csv_value(row.session_file),
        "message_role": _csv_value(row.message_role),
        "message_text": _csv_value(row.message_text),
        "query_text": _csv_value(row.query_text),
        "tool_name": _csv_value(row.tool_name),
        "tool_call_id": _csv_value(row.tool_call_id),
        "tool_start_epoch_ms": _csv_value(row.tool_start_epoch_ms),
        "tool_end_epoch_ms": _csv_value(row.tool_end_epoch_ms),
        "tool_duration_ms": _csv_value(row.tool_duration_ms),
        "input_tokens": _csv_value(row.input_tokens),
        "output_tokens": _csv_value(row.output_tokens),
        "cached_input_tokens": _csv_value(row.cached_input_tokens),
        "reasoning_output_tokens": _csv_value(row.reasoning_output_tokens),
        "total_tokens_raw": _csv_value(row.total_tokens_raw),
        "total_tokens_recomputed": _csv_value(row.total_tokens_recomputed),
        "total_tokens_delta": _csv_value(row.total_tokens_delta),
        "reconciliation_status": _csv_value(row.reconciliation_status),
        "total_tokens": _csv_value(row.total_tokens),
    }


def trim_rows_for_csv_budget(
    rows: list[AuditEventRow],
    *,
    max_retention_days: int,
    max_rows: int,
) -> tuple[list[AuditEventRow], dict[str, Any]]:
    original_count = len(rows)
    trimmed_by_age_limit = 0
    trimmed_by_row_limit = 0
    retention_cutoff_utc = ""
    latest_event_utc = ""

    effective_rows = rows
    latest_ts: datetime | None = None
    if effective_rows:
        latest_ts = effective_rows[-1].timestamp
        latest_event_utc = as_utc_iso(latest_ts)

    if max_retention_days > 0 and latest_ts is not None:
        cutoff_ts = latest_ts - timedelta(days=max_retention_days)
        retention_cutoff_utc = as_utc_iso(cutoff_ts)
        start_idx = 0
        while start_idx < len(effective_rows) and effective_rows[start_idx].timestamp < cutoff_ts:
            start_idx += 1
        trimmed_by_age_limit = start_idx
        effective_rows = effective_rows[start_idx:]

    if max_rows > 0 and len(effective_rows) > max_rows:
        trimmed_by_row_limit = len(effective_rows) - max_rows
        effective_rows = effective_rows[-max_rows:]

    return (
        effective_rows,
        {
            "original_rows": original_count,
            "rows_after_limits": len(effective_rows),
            "trimmed_by_age_limit": trimmed_by_age_limit,
            "trimmed_by_row_limit": trimmed_by_row_limit,
            "max_retention_days": max_retention_days,
            "latest_event_utc": latest_event_utc,
            "retention_cutoff_utc": retention_cutoff_utc,
        },
    )


def write_csv_atomic(
    csv_path: Path,
    rows: list[AuditEventRow],
    *,
    headers: list[str] | None = None,
) -> None:
    fieldnames = headers or CSV_HEADERS
    csv_path.parent.mkdir(parents=True, exist_ok=True)
    with tempfile.NamedTemporaryFile(
        mode="w",
        encoding="utf-8",
        newline="",
        delete=False,
        dir=str(csv_path.parent),
        prefix=".tmp_model_audit_",
        suffix=".csv",
    ) as temp_file:
        writer = csv.DictWriter(temp_file, fieldnames=fieldnames)
        writer.writeheader()
        for row in rows:
            row_payload = _row_to_csv_dict(row)
            writer.writerow({name: row_payload.get(name, "") for name in fieldnames})
        temp_path = Path(temp_file.name)

    temp_path.replace(csv_path)


def write_dataset_csvs_atomic(data_dir: Path, rows: list[AuditEventRow]) -> dict[str, dict[str, Any]]:
    data_dir.mkdir(parents=True, exist_ok=True)
    dataset_meta: dict[str, dict[str, Any]] = {}
    for dataset_key, spec in DATASET_SPECS.items():
        event_type = str(spec["event_type"])
        headers = list(spec["headers"])
        file_name = str(spec["file_name"])
        csv_path = data_dir / file_name
        dataset_rows = [row for row in rows if row.event_type == event_type]
        write_csv_atomic(csv_path, dataset_rows, headers=headers)
        dataset_meta[dataset_key] = {
            "key": dataset_key,
            "event_type": event_type,
            "file_name": file_name,
            "description": str(spec["description"]),
            "required_fields": headers,
            "row_count": len(dataset_rows),
            "csv_path": str(csv_path),
            "relative_path": f"/{DATA_DIR_NAME}/{file_name}",
            "csv_bytes": csv_path.stat().st_size if csv_path.exists() else 0,
        }
    return dataset_meta


def _tool_duration_percentile_ms(sorted_durations: list[int], percentile: int) -> int:
    if not sorted_durations:
        return 0
    if percentile <= 0:
        return sorted_durations[0]
    if percentile >= 100:
        return sorted_durations[-1]

    index = int((percentile / 100) * (len(sorted_durations) - 1))
    return sorted_durations[index]


def build_usage_summary_report(
    rows: list[AuditEventRow],
    *,
    generated_at_utc: str,
    audit_status: str,
) -> dict[str, Any]:
    if not rows:
        return {
            "status": "ok",
            "generated_at_utc": generated_at_utc,
            "audit_status": audit_status,
            "time_window": {"start_utc": "", "end_utc": ""},
            "counts": {
                "rows": 0,
                "threads": 0,
                "queries": 0,
                "models": 0,
                "workspaces": 0,
                "tool_calls": 0,
            },
            "totals": {
                "input_tokens": 0,
                "output_tokens": 0,
                "total_tokens": 0,
                "avg_tokens_per_query": 0.0,
            },
            "by_model": [],
            "by_workspace": [],
            "by_thread": [],
            "by_tool": [],
        }

    input_total = 0
    output_total = 0
    total_tokens = 0

    thread_ids: set[str] = set()
    query_ids: set[str] = set()
    model_ids: set[str] = set()
    workspace_ids: set[str] = set()
    tool_call_count = 0

    model_rollup: dict[str, dict[str, Any]] = {}
    workspace_rollup: dict[str, dict[str, Any]] = {}
    thread_rollup: dict[str, dict[str, Any]] = {}
    tool_rollup: dict[str, dict[str, Any]] = {}

    for row in rows:
        thread_id = str(row.thread_id or "unknown")
        model = str(row.model or "unknown")
        cwd = str(row.cwd or "unknown")
        query_id = str(row.query_id or "").strip()

        if thread_id and thread_id != "unknown":
            thread_ids.add(thread_id)
        if model and model != "unknown":
            model_ids.add(model)
        if cwd and cwd != "unknown":
            workspace_ids.add(cwd)
        if query_id and query_id != "unknown":
            query_ids.add(query_id)

        model_stat = model_rollup.setdefault(
            model,
            {"token_count": 0, "tool_call_count": 0, "query_ids": set(), "thread_ids": set()},
        )
        workspace_stat = workspace_rollup.setdefault(
            cwd,
            {"token_count": 0, "tool_call_count": 0, "query_ids": set(), "thread_ids": set()},
        )
        thread_stat = thread_rollup.setdefault(
            thread_id,
            {
                "thread_label": str(row.thread_label or ""),
                "token_count": 0,
                "tool_call_count": 0,
                "query_ids": set(),
                "last_event_utc": "",
            },
        )
        if not thread_stat["thread_label"] and row.thread_label:
            thread_stat["thread_label"] = str(row.thread_label)
        thread_stat["last_event_utc"] = as_utc_iso(row.timestamp)

        if query_id and query_id != "unknown":
            model_stat["query_ids"].add(query_id)
            model_stat["thread_ids"].add(thread_id)
            workspace_stat["query_ids"].add(query_id)
            workspace_stat["thread_ids"].add(thread_id)
            thread_stat["query_ids"].add(query_id)

        if row.event_type == "token_count":
            if isinstance(row.input_tokens, int):
                input_total += row.input_tokens
            if isinstance(row.output_tokens, int):
                output_total += row.output_tokens
            if isinstance(row.total_tokens, int):
                total_tokens += row.total_tokens
                model_stat["token_count"] += row.total_tokens
                workspace_stat["token_count"] += row.total_tokens
                thread_stat["token_count"] += row.total_tokens

        if row.event_type == "tool_call":
            tool_call_count += 1
            model_stat["tool_call_count"] += 1
            workspace_stat["tool_call_count"] += 1
            thread_stat["tool_call_count"] += 1

            tool_name = str(row.tool_name or "unknown_tool")
            tool_stat = tool_rollup.setdefault(
                tool_name,
                {
                    "call_count": 0,
                    "total_duration_ms": 0,
                    "rows_without_duration": 0,
                    "durations_ms": [],
                },
            )
            tool_stat["call_count"] += 1
            if isinstance(row.tool_duration_ms, int) and row.tool_duration_ms >= 0:
                tool_stat["total_duration_ms"] += row.tool_duration_ms
                tool_stat["durations_ms"].append(row.tool_duration_ms)
            else:
                tool_stat["rows_without_duration"] += 1

    by_model = [
        {
            "model": model,
            "query_count": len(stat["query_ids"]),
            "thread_count": len(stat["thread_ids"]),
            "tool_call_count": int(stat["tool_call_count"]),
            "token_count": int(stat["token_count"]),
        }
        for model, stat in model_rollup.items()
    ]
    by_model.sort(key=lambda item: (-item["token_count"], -item["query_count"], item["model"]))

    by_workspace = [
        {
            "cwd": cwd,
            "query_count": len(stat["query_ids"]),
            "thread_count": len(stat["thread_ids"]),
            "tool_call_count": int(stat["tool_call_count"]),
            "token_count": int(stat["token_count"]),
        }
        for cwd, stat in workspace_rollup.items()
    ]
    by_workspace.sort(key=lambda item: (-item["token_count"], -item["query_count"], item["cwd"]))

    by_thread = [
        {
            "thread_id": thread_id,
            "thread_label": str(stat["thread_label"] or ""),
            "query_count": len(stat["query_ids"]),
            "tool_call_count": int(stat["tool_call_count"]),
            "token_count": int(stat["token_count"]),
            "last_event_utc": str(stat["last_event_utc"] or ""),
        }
        for thread_id, stat in thread_rollup.items()
    ]
    by_thread.sort(key=lambda item: (-item["token_count"], -item["query_count"], item["thread_id"]))

    by_tool = []
    for tool_name, stat in tool_rollup.items():
        call_count = int(stat["call_count"])
        total_duration_ms = int(stat["total_duration_ms"])
        durations = sorted(int(value) for value in stat["durations_ms"])
        avg_duration_ms = int(total_duration_ms / call_count) if call_count > 0 else 0
        by_tool.append(
            {
                "tool_name": tool_name,
                "call_count": call_count,
                "total_duration_ms": total_duration_ms,
                "avg_duration_ms": avg_duration_ms,
                "p95_duration_ms": _tool_duration_percentile_ms(durations, 95),
                "rows_without_duration": int(stat["rows_without_duration"]),
            }
        )
    by_tool.sort(key=lambda item: (-item["call_count"], item["tool_name"]))

    start_utc = as_utc_iso(rows[0].timestamp)
    end_utc = as_utc_iso(rows[-1].timestamp)
    query_count = len(query_ids)
    avg_tokens_per_query = (total_tokens / query_count) if query_count else 0.0

    return {
        "status": "ok",
        "generated_at_utc": generated_at_utc,
        "audit_status": audit_status,
        "time_window": {"start_utc": start_utc, "end_utc": end_utc},
        "counts": {
            "rows": len(rows),
            "threads": len(thread_ids),
            "queries": query_count,
            "models": len(model_ids),
            "workspaces": len(workspace_ids),
            "tool_calls": tool_call_count,
        },
        "totals": {
            "input_tokens": input_total,
            "output_tokens": output_total,
            "total_tokens": total_tokens,
            "avg_tokens_per_query": avg_tokens_per_query,
        },
        "by_model": by_model,
        "by_workspace": by_workspace,
        "by_thread": by_thread,
        "by_tool": by_tool,
    }


def _normalize_query_signature(text: str) -> str:
    normalized = text.strip().lower()
    if not normalized:
        return ""
    normalized = UUID_PATTERN.sub("<id>", normalized)
    normalized = NUMERIC_PATTERN.sub("<num>", normalized)
    normalized = SPACE_PATTERN.sub(" ", normalized)
    return normalized.strip()


def _query_index_for_row(row: AuditEventRow) -> int | None:
    if isinstance(row.query_index, int) and row.query_index > 0:
        return row.query_index
    query_id = str(row.query_id or "")
    match = QUERY_INDEX_PATTERN.search(query_id)
    if not match:
        return None
    try:
        value = int(match.group(1))
    except ValueError:
        return None
    return value if value > 0 else None


def _mean(values: list[float]) -> float:
    if not values:
        return 0.0
    return sum(values) / len(values)


def _stddev(values: list[float], mean_value: float) -> float:
    if not values:
        return 0.0
    return (_mean([(value - mean_value) ** 2 for value in values])) ** 0.5


def _regression_slope(values: list[float]) -> float:
    n = len(values)
    if n < 2:
        return 0.0
    xs = [float(index + 1) for index in range(n)]
    x_mean = _mean(xs)
    y_mean = _mean(values)
    numerator = 0.0
    denominator = 0.0
    for index in range(n):
        dx = xs[index] - x_mean
        numerator += dx * (values[index] - y_mean)
        denominator += dx * dx
    if denominator == 0.0:
        return 0.0
    return numerator / denominator


def _percentile(values: list[int], percentile: int) -> int:
    if not values:
        return 0
    if percentile <= 0:
        return min(values)
    if percentile >= 100:
        return max(values)
    ordered = sorted(values)
    index = int((percentile / 100) * len(ordered))
    index = max(0, min(index, len(ordered) - 1))
    return ordered[index]


def _opportunity_severity(estimated_token_savings: int) -> str:
    if estimated_token_savings >= 5000:
        return "high"
    if estimated_token_savings >= 1500:
        return "medium"
    return "low"


def build_optimization_report(rows: list[AuditEventRow], *, generated_at_utc: str) -> dict[str, Any]:
    thread_label_by_id: dict[str, str] = {}
    for row in rows:
        thread_id = str(row.thread_id or "unknown")
        thread_label = str(row.thread_label or "").strip()
        if thread_id and thread_label and thread_id not in thread_label_by_id:
            thread_label_by_id[thread_id] = thread_label

    token_rows = [row for row in rows if row.event_type == "token_count"]
    query_rows = [row for row in rows if row.event_type == "user_message"]

    query_total_tokens: dict[str, int] = {}
    thread_query_input_tokens: dict[str, dict[int, int]] = {}
    for row in token_rows:
        query_id = str(row.query_id or "").strip()
        if query_id and query_id != "unknown":
            query_total_tokens[query_id] = query_total_tokens.get(query_id, 0) + max(0, int(row.total_tokens or 0))

        query_index = _query_index_for_row(row)
        if query_index is None:
            continue
        thread_id = str(row.thread_id or "unknown")
        if thread_id not in thread_query_input_tokens:
            thread_query_input_tokens[thread_id] = {}
        thread_map = thread_query_input_tokens[thread_id]
        thread_map[query_index] = thread_map.get(query_index, 0) + max(0, int(row.input_tokens or 0))

    query_meta: dict[str, dict[str, Any]] = {}
    for row in query_rows:
        query_id = str(row.query_id or "").strip()
        if not query_id or query_id == "unknown":
            continue
        query_text = str(row.query_text or row.message_text or "").strip()
        if not query_text:
            continue
        if query_id in query_meta:
            continue
        query_meta[query_id] = {
            "query_text": query_text,
            "thread_id": str(row.thread_id or "unknown"),
            "cwd": str(row.cwd or "unknown"),
        }

    opportunities: list[dict[str, Any]] = []

    repeat_groups: dict[str, dict[str, Any]] = {}
    for query_id, meta in query_meta.items():
        signature = _normalize_query_signature(str(meta["query_text"]))
        if not signature:
            continue
        total_tokens = int(query_total_tokens.get(query_id, 0))
        if total_tokens <= 0:
            continue
        group = repeat_groups.setdefault(
            signature,
            {
                "occurrences": 0,
                "total_tokens": 0,
                "max_single_tokens": 0,
                "query_text": str(meta["query_text"]),
                "workspaces": set(),
                "threads": set(),
            },
        )
        group["occurrences"] += 1
        group["total_tokens"] += total_tokens
        group["max_single_tokens"] = max(int(group["max_single_tokens"]), total_tokens)
        group["workspaces"].add(str(meta["cwd"] or "unknown"))
        group["threads"].add(str(meta["thread_id"] or "unknown"))

    repeat_candidates: list[dict[str, Any]] = []
    for signature, group in repeat_groups.items():
        occurrences = int(group["occurrences"])
        if occurrences < 3:
            continue
        total_tokens = int(group["total_tokens"])
        avoidable_tokens = total_tokens - int(group["max_single_tokens"])
        if avoidable_tokens <= 0:
            continue
        repeat_candidates.append(
            {
                "signature": signature,
                "signature_preview": str(group["query_text"])[:120],
                "occurrences": occurrences,
                "total_tokens": total_tokens,
                "avoidable_tokens": avoidable_tokens,
                "workspace_count": len(group["workspaces"]),
                "thread_count": len(group["threads"]),
            }
        )

    repeat_candidates.sort(key=lambda item: (-int(item["avoidable_tokens"]), -int(item["occurrences"])))
    if repeat_candidates:
        top_repeats = repeat_candidates[:3]
        repeat_savings = sum(int(item["avoidable_tokens"]) for item in top_repeats)
        opportunities.append(
            {
                "id": "repeat_query_waste",
                "category": "query_efficiency",
                "severity": _opportunity_severity(repeat_savings),
                "title": "Repeated prompts are driving avoidable token spend",
                "estimated_token_savings": repeat_savings,
                "confidence": "high",
                "evidence": {
                    "repeat_signature_count": len(repeat_candidates),
                    "top_signatures": top_repeats,
                },
                "actions": [
                    "Template repeated prompts and reuse a stable shared prompt instead of retyping variants.",
                    "Move recurring setup context into a compact reusable preamble to shrink repeated input tokens.",
                    "Consolidate duplicate prompt loops across workspaces into one reusable workflow.",
                ],
            }
        )

    context_candidates: list[dict[str, Any]] = []
    for thread_id, index_map in thread_query_input_tokens.items():
        ordered = [value for _, value in sorted(index_map.items()) if value >= 0]
        if len(ordered) < 6:
            continue

        rolling: list[float] = []
        for idx in range(len(ordered)):
            start_idx = max(0, idx - 4)
            window_values = [float(value) for value in ordered[start_idx : idx + 1]]
            rolling.append(_mean(window_values))

        slope = _regression_slope(rolling[-10:])
        midpoint = max(1, len(rolling) // 2)
        early_mean = _mean(rolling[:midpoint])
        late_mean = _mean(rolling[midpoint:])
        latest = rolling[-1]

        if slope < 25.0:
            continue
        if latest < 300.0:
            continue
        if late_mean <= (early_mean * 1.2):
            continue

        estimated_savings = int(max(0.0, late_mean - early_mean) * max(1, len(ordered) // 2))
        if estimated_savings <= 0:
            continue

        context_candidates.append(
            {
                "thread_id": thread_id,
                "thread_label": thread_label_by_id.get(thread_id, ""),
                "query_count": len(ordered),
                "slope": round(slope, 3),
                "latest_rolling_input_tokens": int(round(latest)),
                "estimated_token_savings": estimated_savings,
            }
        )

    context_candidates.sort(key=lambda item: (-int(item["estimated_token_savings"]), -float(item["slope"])))
    if context_candidates:
        top_context = context_candidates[:3]
        context_savings = sum(int(item["estimated_token_savings"]) for item in top_context)
        opportunities.append(
            {
                "id": "context_bloat_threads",
                "category": "context_management",
                "severity": _opportunity_severity(context_savings),
                "title": "Long-running threads show context bloat",
                "estimated_token_savings": context_savings,
                "confidence": "medium",
                "evidence": {
                    "thread_count_flagged": len(context_candidates),
                    "top_threads": top_context,
                },
                "actions": [
                    "Split long threads once rolling input rises sharply to avoid carrying stale context.",
                    "Ask the model to summarize state and continue from the summary in a fresh thread.",
                    "Promote durable facts into lightweight checklists rather than restating full background each turn.",
                ],
            }
        )

    bucket_ms = 60 * 60 * 1000
    bucket_map: dict[int, dict[str, Any]] = {}
    for row in token_rows:
        ts_ms = int(row.timestamp.timestamp() * 1000)
        bucket_ts = (ts_ms // bucket_ms) * bucket_ms
        bucket = bucket_map.setdefault(bucket_ts, {"ts": bucket_ts, "total": 0, "by_workspace": {}, "by_thread": {}})
        total = max(0, int(row.total_tokens or 0))
        bucket["total"] = int(bucket["total"]) + total
        workspace = str(row.cwd or "unknown")
        bucket["by_workspace"][workspace] = int(bucket["by_workspace"].get(workspace, 0)) + total
        thread_id = str(row.thread_id or "unknown")
        bucket["by_thread"][thread_id] = int(bucket["by_thread"].get(thread_id, 0)) + total

    sorted_buckets = [bucket_map[key] for key in sorted(bucket_map)]
    bucket_totals = [int(item["total"]) for item in sorted_buckets]
    p90 = _percentile(bucket_totals, 90)
    spike_events: list[dict[str, Any]] = []
    for idx in range(12, len(sorted_buckets)):
        history = [int(value["total"]) for value in sorted_buckets[idx - 12 : idx]]
        mean_history = _mean([float(value) for value in history])
        std_history = _stddev([float(value) for value in history], mean_history)
        current_total = int(sorted_buckets[idx]["total"])
        z_score = 0.0
        if std_history > 0:
            z_score = (current_total - mean_history) / std_history
        elif current_total > mean_history:
            z_score = 999.0

        if z_score < 3.0:
            continue
        if current_total < p90:
            continue
        if current_total < 500:
            continue

        workspace_items = sorted(
            sorted_buckets[idx]["by_workspace"].items(),
            key=lambda item: int(item[1]),
            reverse=True,
        )
        thread_items = sorted(
            sorted_buckets[idx]["by_thread"].items(),
            key=lambda item: int(item[1]),
            reverse=True,
        )
        top_workspace = workspace_items[0] if workspace_items else ("unknown", 0)
        top_thread = thread_items[0] if thread_items else ("unknown", 0)
        spike_events.append(
            {
                "bucket_utc": datetime.fromtimestamp(
                    int(sorted_buckets[idx]["ts"]) / 1000, tz=timezone.utc
                ).isoformat().replace("+00:00", "Z"),
                "total_tokens": current_total,
                "baseline_mean_tokens": int(round(mean_history)),
                "z_score": round(z_score, 2),
                "top_workspace": str(top_workspace[0]),
                "top_thread_id": str(top_thread[0]),
                "top_thread_label": thread_label_by_id.get(str(top_thread[0]), ""),
                "excess_tokens": max(0, int(round(current_total - mean_history))),
            }
        )

    if spike_events:
        spike_savings = int(sum(int(item["excess_tokens"]) * 0.2 for item in spike_events))
        if spike_savings > 0:
            opportunities.append(
                {
                    "id": "token_spike_guardrails",
                    "category": "burst_control",
                    "severity": _opportunity_severity(spike_savings),
                    "title": "Burst windows indicate preventable token spikes",
                    "estimated_token_savings": spike_savings,
                    "confidence": "medium",
                    "evidence": {
                        "spike_count": len(spike_events),
                        "top_spikes": spike_events[:5],
                    },
                    "actions": [
                        "Set a per-hour soft token budget and pause low-priority threads after budget breach.",
                        "Batch similar asks inside one prompt to avoid duplicate high-load windows.",
                        "Run heavy analysis prompts during quieter periods to reduce concurrent context inflation.",
                    ],
                }
            )

    opportunities.sort(key=lambda item: (-int(item["estimated_token_savings"]), item["id"]))
    for idx, opportunity in enumerate(opportunities, start=1):
        opportunity["priority"] = idx

    time_window = {"start_utc": "", "end_utc": ""}
    if rows:
        time_window = {"start_utc": as_utc_iso(rows[0].timestamp), "end_utc": as_utc_iso(rows[-1].timestamp)}

    return {
        "status": "ok",
        "generated_at_utc": generated_at_utc,
        "time_window": time_window,
        "summary": {
            "opportunity_count": len(opportunities),
            "estimated_total_token_savings": sum(int(item["estimated_token_savings"]) for item in opportunities),
            "top_opportunity_id": opportunities[0]["id"] if opportunities else "",
            "token_row_count": len(token_rows),
            "query_row_count": len(query_rows),
        },
        "opportunities": opportunities,
    }


class AuditAppContext:
    def __init__(
        self,
        sessions_dir: Path,
        reports_dir: Path,
        *,
        max_retention_days: int = DEFAULT_MAX_RETENTION_DAYS,
        max_csv_rows: int = DEFAULT_MAX_CSV_ROWS,
    ) -> None:
        self.sessions_dir = sessions_dir
        self.reports_dir = reports_dir
        self.html_path = reports_dir / HTML_NAME
        self.data_dir = reports_dir / DATA_DIR_NAME
        self.data_catalog_path = self.data_dir / DATA_CATALOG_NAME
        self.audit_json_path = reports_dir / AUDIT_JSON_NAME
        self.summary_json_path = reports_dir / SUMMARY_JSON_NAME
        self.opportunities_json_path = reports_dir / OPPORTUNITIES_JSON_NAME
        self.max_retention_days = max_retention_days
        self.max_csv_rows = max_csv_rows
        self.last_refresh_meta: dict[str, Any] | None = None

    def refresh_csv(self) -> dict[str, Any]:
        generated_at = datetime.now(timezone.utc)
        generated_at_utc = as_utc_iso(generated_at)
        rows = extract_event_rows(self.sessions_dir)
        rows, trim_stats = trim_rows_for_csv_budget(
            rows,
            max_retention_days=self.max_retention_days,
            max_rows=self.max_csv_rows,
        )
        dataset_meta = write_dataset_csvs_atomic(self.data_dir, rows)
        total_data_bytes = sum(int(item.get("csv_bytes") or 0) for item in dataset_meta.values())

        latest_event_epoch_ms = 0
        latest_event_utc = str(trim_stats.get("latest_event_utc", ""))
        if latest_event_utc:
            latest_dt = parse_timestamp(latest_event_utc)
            if latest_dt is not None:
                latest_event_epoch_ms = int(latest_dt.timestamp() * 1000)

        audit_report = build_usage_audit_report(rows=rows, generated_at_utc=generated_at_utc)
        self.audit_json_path.write_text(
            json.dumps(audit_report, ensure_ascii=True, indent=2),
            encoding="utf-8",
        )
        summary_report = build_usage_summary_report(
            rows=rows,
            generated_at_utc=generated_at_utc,
            audit_status=str(audit_report.get("status", "unknown")),
        )
        self.summary_json_path.write_text(
            json.dumps(summary_report, ensure_ascii=True, indent=2),
            encoding="utf-8",
        )
        opportunities_report = build_optimization_report(rows=rows, generated_at_utc=generated_at_utc)
        self.opportunities_json_path.write_text(
            json.dumps(opportunities_report, ensure_ascii=True, indent=2),
            encoding="utf-8",
        )

        data_catalog = {
            "status": "ok",
            "generated_at_utc": generated_at_utc,
            "latest_event_utc": latest_event_utc,
            "latest_event_epoch_ms": latest_event_epoch_ms,
            "source_sessions_dir": str(self.sessions_dir),
            "max_retention_days": self.max_retention_days,
            "retention_cutoff_utc": trim_stats["retention_cutoff_utc"],
            "datasets": list(dataset_meta.values()),
        }
        self.data_catalog_path.write_text(
            json.dumps(data_catalog, ensure_ascii=True, indent=2),
            encoding="utf-8",
        )

        message: str | None = None
        if not self.sessions_dir.exists():
            message = f"Sessions directory does not exist: {self.sessions_dir}"
        elif not rows:
            message = "No supported events found in sessions logs."

        meta = {
            "status": "ok",
            "rows_written": len(rows),
            "rows_source_total": trim_stats["original_rows"],
            "rows_trimmed_total": trim_stats["trimmed_by_age_limit"] + trim_stats["trimmed_by_row_limit"],
            "rows_trimmed_by_age_limit": trim_stats["trimmed_by_age_limit"],
            "rows_trimmed_by_row_limit": trim_stats["trimmed_by_row_limit"],
            "max_retention_days": self.max_retention_days,
            "retention_cutoff_utc": trim_stats["retention_cutoff_utc"],
            "latest_event_utc": trim_stats["latest_event_utc"],
            "max_csv_rows": self.max_csv_rows,
            "estimated_csv_bytes": total_data_bytes,
            "generated_at_utc": generated_at_utc,
            "source_sessions_dir": str(self.sessions_dir),
            "data_dir": str(self.data_dir),
            "data_catalog_path": str(self.data_catalog_path),
            "audit_json_path": str(self.audit_json_path),
            "summary_json_path": str(self.summary_json_path),
            "opportunities_json_path": str(self.opportunities_json_path),
            "audit_status": str(audit_report.get("status", "unknown")),
            "metrics_version": str(audit_report.get("metrics_version", "unknown")),
            "latest_event_epoch_ms": latest_event_epoch_ms,
            "dataset_rows": {key: int(value["row_count"]) for key, value in dataset_meta.items()},
            "opportunities_count": int(opportunities_report["summary"]["opportunity_count"]),
            "estimated_total_token_savings": int(opportunities_report["summary"]["estimated_total_token_savings"]),
        }
        if message:
            meta["message"] = message
        self.last_refresh_meta = meta
        return meta

    def load_dashboard_html(self) -> str:
        text = self.html_path.read_text(encoding="utf-8")
        refresh_time = ""
        if self.last_refresh_meta:
            refresh_time = str(self.last_refresh_meta.get("generated_at_utc", ""))
        text = text.replace("__SOURCE_SESSIONS_DIR__", str(self.sessions_dir))
        text = text.replace("__DATA_BASE_PATH__", f"/{DATA_DIR_NAME}")
        text = text.replace("__AUDIT_JSON_RELATIVE_PATH__", "/audit.json")
        text = text.replace("__OPPORTUNITIES_JSON_RELATIVE_PATH__", "/opportunities.json")
        text = text.replace("__LAST_REFRESH_UTC__", refresh_time)
        return text

    def ensure_data(self) -> dict[str, Any]:
        has_csv = False
        for spec in DATASET_SPECS.values():
            file_name = str(spec["file_name"])
            if (self.data_dir / file_name).exists():
                has_csv = True
                break
        has_core_artifacts = (
            self.data_catalog_path.exists()
            and self.audit_json_path.exists()
            and self.summary_json_path.exists()
            and self.opportunities_json_path.exists()
        )
        if has_csv and has_core_artifacts:
            return self.last_refresh_meta or {"status": "ok"}
        return self.refresh_csv()

    def read_dataset_csv(
        self,
        dataset_file_name: str,
        *,
        start_ms: int | None = None,
        end_ms: int | None = None,
    ) -> bytes:
        csv_path = self.data_dir / dataset_file_name
        if not csv_path.exists():
            raise FileNotFoundError(f"Dataset CSV missing: {csv_path}")
        if start_ms is None and end_ms is None:
            return csv_path.read_bytes()

        with csv_path.open("r", encoding="utf-8", newline="") as handle:
            reader = csv.DictReader(handle)
            fieldnames = list(reader.fieldnames or [])
            output = tempfile.NamedTemporaryFile(
                mode="w",
                encoding="utf-8",
                newline="",
                delete=False,
                dir=str(csv_path.parent),
                prefix=".tmp_model_audit_window_",
                suffix=".csv",
            )
            temp_path = Path(output.name)
            try:
                writer = csv.DictWriter(output, fieldnames=fieldnames)
                writer.writeheader()
                for row in reader:
                    ts_text = row.get("timestamp_epoch_ms")
                    if not ts_text:
                        continue
                    try:
                        ts = int(ts_text)
                    except ValueError:
                        continue
                    if start_ms is not None and ts < start_ms:
                        continue
                    if end_ms is not None and ts > end_ms:
                        continue
                    writer.writerow(row)
            finally:
                output.close()

        try:
            payload = temp_path.read_bytes()
        finally:
            temp_path.unlink(missing_ok=True)
        return payload


class AuditDashboardHandler(BaseHTTPRequestHandler):
    server_version = "ModelAuditLocalApp/2.0"

    @property
    def app(self) -> AuditAppContext:
        return self.server.app_context  # type: ignore[attr-defined]

    def _send_bytes(
        self,
        code: int,
        body: bytes,
        content_type: str,
        *,
        cache_control: str = "no-store",
    ) -> None:
        self.send_response(code)
        self.send_header("Content-Type", content_type)
        self.send_header("Content-Length", str(len(body)))
        self.send_header("Cache-Control", cache_control)
        self.end_headers()
        self.wfile.write(body)

    def _send_json(self, code: int, payload: dict[str, Any]) -> None:
        body = json.dumps(payload, ensure_ascii=True).encode("utf-8")
        self._send_bytes(code, body, "application/json; charset=utf-8")

    def _send_not_found(self) -> None:
        self._send_json(
            HTTPStatus.NOT_FOUND,
            {"status": "error", "message": "Not found"},
        )

    def log_message(self, format: str, *args: Any) -> None:  # noqa: A003
        sys.stdout.write(
            "%s - - [%s] %s\n"
            % (
                self.address_string(),
                self.log_date_time_string(),
                format % args,
            )
        )

    def do_GET(self) -> None:  # noqa: N802
        parsed = urlparse(self.path)
        if parsed.path == "/":
            try:
                html = self.app.load_dashboard_html().encode("utf-8")
            except FileNotFoundError:
                self._send_json(
                    HTTPStatus.INTERNAL_SERVER_ERROR,
                    {
                        "status": "error",
                        "message": f"Dashboard template missing: {self.app.html_path}",
                    },
                )
                return
            self._send_bytes(HTTPStatus.OK, html, "text/html; charset=utf-8")
            return

        if parsed.path.startswith(f"/{DATA_DIR_NAME}/"):
            meta = self.app.ensure_data()
            if meta.get("status") != "ok":
                self._send_json(HTTPStatus.INTERNAL_SERVER_ERROR, meta)
                return

            relative = parsed.path[len(f"/{DATA_DIR_NAME}/") :]
            if relative == DATA_CATALOG_NAME:
                body = self.app.data_catalog_path.read_bytes()
                self._send_bytes(HTTPStatus.OK, body, "application/json; charset=utf-8")
                return

            allowed_files = {str(spec["file_name"]) for spec in DATASET_SPECS.values()}
            if relative not in allowed_files:
                self._send_not_found()
                return

            qs = parse_qs(parsed.query)

            def _query_int(name: str) -> int | None:
                raw = qs.get(name, [""])[0].strip()
                if not raw:
                    return None
                try:
                    return int(raw)
                except ValueError as exc:
                    raise ValueError(f"Invalid query parameter '{name}': {raw}") from exc

            try:
                start_ms = _query_int("start_ms")
                end_ms = _query_int("end_ms")
                body = self.app.read_dataset_csv(relative, start_ms=start_ms, end_ms=end_ms)
            except FileNotFoundError:
                self._send_not_found()
                return
            except ValueError as exc:
                self._send_json(HTTPStatus.BAD_REQUEST, {"status": "error", "message": str(exc)})
                return

            self._send_bytes(HTTPStatus.OK, body, "text/csv; charset=utf-8")
            return

        if parsed.path == "/audit.json":
            if not self.app.audit_json_path.exists():
                meta = self.app.ensure_data()
                if meta.get("status") != "ok":
                    self._send_json(HTTPStatus.INTERNAL_SERVER_ERROR, meta)
                    return
            body = self.app.audit_json_path.read_bytes()
            self._send_bytes(HTTPStatus.OK, body, "application/json; charset=utf-8")
            return

        if parsed.path == "/summary.json":
            if not self.app.summary_json_path.exists():
                meta = self.app.ensure_data()
                if meta.get("status") != "ok":
                    self._send_json(HTTPStatus.INTERNAL_SERVER_ERROR, meta)
                    return
            body = self.app.summary_json_path.read_bytes()
            self._send_bytes(HTTPStatus.OK, body, "application/json; charset=utf-8")
            return

        if parsed.path == "/opportunities.json":
            if not self.app.opportunities_json_path.exists():
                meta = self.app.ensure_data()
                if meta.get("status") != "ok":
                    self._send_json(HTTPStatus.INTERNAL_SERVER_ERROR, meta)
                    return
            body = self.app.opportunities_json_path.read_bytes()
            self._send_bytes(HTTPStatus.OK, body, "application/json; charset=utf-8")
            return

        self._send_not_found()

    def do_POST(self) -> None:  # noqa: N802
        parsed = urlparse(self.path)
        if parsed.path == "/refresh":
            try:
                meta = self.app.refresh_csv()
            except Exception as exc:  # noqa: BLE001
                self._send_json(
                    HTTPStatus.INTERNAL_SERVER_ERROR,
                    {"status": "error", "message": f"Refresh failed: {exc}"},
                )
                return
            self._send_json(HTTPStatus.OK, meta)
            return

        self._send_not_found()


def parse_args() -> argparse.Namespace:
    default_reports_dir = Path(__file__).resolve().parent
    parser = argparse.ArgumentParser(
        description="Run local model audit mini app (typed CSV-backed dashboard).",
    )
    parser.add_argument("--host", default="127.0.0.1", help="Bind host (default: 127.0.0.1)")
    parser.add_argument("--port", type=int, default=8765, help="Bind port (default: 8765)")
    parser.add_argument(
        "--sessions-dir",
        default=str(Path.home() / ".codex" / "sessions"),
        help="Codex sessions directory (default: ~/.codex/sessions)",
    )
    parser.add_argument(
        "--reports-dir",
        default=str(default_reports_dir),
        help=(
            f"Reports directory containing {HTML_NAME}, {AUDIT_JSON_NAME}, {SUMMARY_JSON_NAME}, "
            f"{OPPORTUNITIES_JSON_NAME}, and {DATA_DIR_NAME}/"
        ),
    )
    parser.add_argument(
        "--max-retention-days",
        type=int,
        default=DEFAULT_MAX_RETENTION_DAYS,
        help=f"Maximum age of events in days retained in typed CSV datasets (default: {DEFAULT_MAX_RETENTION_DAYS})",
    )
    parser.add_argument(
        "--max-csv-rows",
        type=int,
        default=DEFAULT_MAX_CSV_ROWS,
        help=(
            "Optional hard row cap after age retention (default: 0 = disabled, no row truncation). "
            f"Current default: {DEFAULT_MAX_CSV_ROWS}"
        ),
    )
    return parser.parse_args()


def main() -> int:
    args = parse_args()
    sessions_dir = Path(args.sessions_dir).expanduser().resolve()
    reports_dir = Path(args.reports_dir).expanduser().resolve()

    app_context = AuditAppContext(
        sessions_dir=sessions_dir,
        reports_dir=reports_dir,
        max_retention_days=max(0, args.max_retention_days),
        max_csv_rows=max(0, args.max_csv_rows),
    )
    if not app_context.html_path.exists():
        raise SystemExit(f"Missing dashboard template: {app_context.html_path}")

    startup_meta = app_context.refresh_csv()
    print(
        "[model-audit] startup refresh: "
        f"rows={startup_meta['rows_written']} "
        f"trimmed={startup_meta['rows_trimmed_total']} "
        f"retention_days={startup_meta['max_retention_days']} "
        f"estimated_csv_bytes={startup_meta['estimated_csv_bytes']} "
        f"generated_at={startup_meta['generated_at_utc']}"
    )
    print(f"[model-audit] source sessions: {sessions_dir}")
    print(f"[model-audit] data dir: {app_context.data_dir}")

    server = ThreadingHTTPServer((args.host, args.port), AuditDashboardHandler)
    server.app_context = app_context  # type: ignore[attr-defined]

    print(f"[model-audit] serving at http://{args.host}:{args.port}/")
    try:
        server.serve_forever()
    except KeyboardInterrupt:
        pass
    finally:
        server.server_close()
    return 0


if __name__ == "__main__":
    raise SystemExit(main())
